{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "290ba113-fd34-452f-85d6-0ad769630bfc",
   "metadata": {},
   "source": [
    "#DSI-Abschlussprojekt\n",
    "#Weltmeere - Auswirkung von Wassertemperatur und Salzgehalt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f004629d-4b08-44da-b33d-bde8f57299e0",
   "metadata": {},
   "source": [
    "##Gruppenmitglieder\n",
    "Bayr Klemens, BSc.\n",
    "Hufnagl Ivo, BSc.\n",
    "Pribil Nadine BSc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3157639-f040-4061-bd03-c6c24b558288",
   "metadata": {},
   "source": [
    "##Geplante Datenquellen\n",
    "Flatfiles\n",
    "- API von ESA\n",
    "- https://climate.esa.int/de/odp/#/dashboard\n",
    "\n",
    "API Data\n",
    "- Amentum\n",
    "- https://developer.amentum.io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f3459c-346a-4c3e-8311-c3de27c6625d",
   "metadata": {},
   "source": [
    "##Geplante Datensicherung\n",
    "- GIT Repository\n",
    "- Datenbanksicherung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0676c078-0af0-409f-a1dd-5259f0010faa",
   "metadata": {},
   "source": [
    "##Geplante Vorgehensweise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dcbb0a-e806-48ed-8807-6050642833d1",
   "metadata": {},
   "source": [
    "###Schritt 1: Datenfindung\n",
    "Recherche von ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68504ed3-aeca-4c5e-a3d4-6dab119cb20c",
   "metadata": {},
   "source": [
    "###Schritt 2: Datenbeschaffung\n",
    "Mindestens 2 verschiedene Datenquellen, die miteinander verbunden sind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4ca255-8b3a-45a0-96c7-fe548c7511eb",
   "metadata": {},
   "source": [
    "###Schritt 3: Datenanalyse\n",
    "Die Daten wollen wir in eine Datenbank spielen. Eventuell CouchDB. Die Daten werden wir mit Kafka bereitstellen und für die Analyse wird dann SparkSQL verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51bb136-e94b-4963-90eb-3602cef3992a",
   "metadata": {},
   "source": [
    "###Schritt 4: Ergebnisdarstellung\n",
    "Die erwarteten Ergebnisse werden mit Grafiken veranschaulicht. Diese werden wir mit Phyton erstellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e04a6-51ae-47d4-917f-0888fc8aa17d",
   "metadata": {},
   "source": [
    "##Weitere Eckpunkte\n",
    "- Speichern / Lesen / Verarbeiten der Daten mit einer Datenbank (relational oder NoSQL)\n",
    "- Kafka für die Bereitstellung der Daten verwenden (zumindest einen Teil, Kafka Producer, Kafka Consumer)\n",
    "- Spark für Datenhandling/analyse verwenden.\n",
    "- Abbildung von MapReduce mit Spark RDDs.\n",
    "- SparkSQL mindestens einmal verwenden.\n",
    "- Spark Dataframes verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652134a7-5b48-4ae4-85fe-126be0329225",
   "metadata": {},
   "source": [
    "##Meilensteine\n",
    "1. 14.12.2022: Präsentation des Themas und der Vorgehensweise.\n",
    "2. 21.12.2022: Zwischenabgabe des Projekts und Feedback-Einholung.\n",
    "3. 25.01.2023: Abgabe und Präsentation der Endergebnisse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a1da8-2a76-4564-be41-4a7d4eb1d532",
   "metadata": {},
   "source": [
    "##Erwarteter Output\n",
    "Grafiken..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "589fc544-0b3e-4872-8ab6-54cd50d60d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Using cached kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e3d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer, KafkaProducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15de804e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"current_u\": {\n",
      "        \"units\": \"m/s\",\n",
      "        \"value\": 0.050109099596738815\n",
      "    },\n",
      "    \"current_v\": {\n",
      "        \"units\": \"m/s\",\n",
      "        \"value\": -0.2798057496547699\n",
      "    },\n",
      "    \"point\": {\n",
      "        \"depth\": 30.0,\n",
      "        \"latitude\": -34.04350280761719,\n",
      "        \"longitude\": 151.977294921875\n",
      "    },\n",
      "    \"salinity\": {\n",
      "        \"units\": \"g/kg\",\n",
      "        \"value\": 35.560813903808594\n",
      "    },\n",
      "    \"temperature\": {\n",
      "        \"units\": \"deg C\",\n",
      "        \"value\": 21.43215560913086\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "url = \"https://ocean.amentum.io/rtofs\"\n",
    "\n",
    "headers = {\"API-Key\": \"SSDqrYzBLYFDuO0iEnCKnYWdQLWNcGSO\"}\n",
    "\n",
    "params = {\n",
    "  \"latitude\": -34.0,\n",
    "  \"longitude\": 152.0,\n",
    "  \"depth\": 30\n",
    "}\n",
    "\n",
    "# handle exceptions\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "json_payload = response.json()\n",
    "\n",
    "print(json.dumps(json_payload, indent=4, sort_keys=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dadcaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to Kafka and open producer\n",
    "from json import dumps\n",
    "group_name = \"rtofs\"\n",
    "topic_name = \"pos1\"\n",
    "servers = ['localhost:29092']  # has to be adapted\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=servers,\n",
    "                         value_serializer=lambda x:dumps(x).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d10fe92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data sent to topic pos1  {'current_u': {'units': 'm/s', 'value': 0.050109099596738815}, 'current_v': {'units': 'm/s', 'value': -0.2798057496547699}, 'point': {'depth': 30.0, 'latitude': -34.04350280761719, 'longitude': 151.977294921875}, 'salinity': {'units': 'g/kg', 'value': 35.560813903808594}, 'temperature': {'units': 'deg C', 'value': 21.43215560913086}}\n"
     ]
    }
   ],
   "source": [
    "# write data into Kafka\n",
    "data = json_payload\n",
    "producer.send(topic_name, value=data)\n",
    "print (\"data sent to topic \"+topic_name+\" \",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8ae8262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "# connect to Kafka and open consumer\n",
    "group_name = \"rtofs\"\n",
    "topic_name = \"pos1\"\n",
    "servers = ['localhost:29092']  # has to be adapted\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "     topic_name,\n",
    "     bootstrap_servers=servers,\n",
    "     auto_offset_reset='earliest',\n",
    "     auto_commit_interval_ms=1000,\n",
    "     enable_auto_commit=True,\n",
    "     group_id=group_name,\n",
    "     value_deserializer=lambda x: loads(x.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1ef7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'current_u': {'units': 'm/s', 'value': 0.050109099596738815}, 'current_v': {'units': 'm/s', 'value': -0.2798057496547699}, 'point': {'depth': 30.0, 'latitude': -34.04350280761719, 'longitude': 151.977294921875}, 'salinity': {'units': 'g/kg', 'value': 35.560813903808594}, 'temperature': {'units': 'deg C', 'value': 21.43215560913086}} read from topic pos1\n"
     ]
    }
   ],
   "source": [
    "# read data from Kafka in a loop\n",
    "for message in consumer:\n",
    "    data = message.value\n",
    "    print('{} read from topic {}'.format(data, topic_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6806c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
