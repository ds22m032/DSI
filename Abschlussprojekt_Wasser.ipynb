{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "290ba113-fd34-452f-85d6-0ad769630bfc",
   "metadata": {},
   "source": [
    "#DSI-Abschlussprojekt\n",
    "#Weltmeere - Auswirkung von Wassertemperatur und Salzgehalt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f004629d-4b08-44da-b33d-bde8f57299e0",
   "metadata": {},
   "source": [
    "##Gruppenmitglieder\n",
    "Bayr Klemens, BSc.\n",
    "Hufnagl Ivo, BSc.\n",
    "Pribil Nadine BSc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3157639-f040-4061-bd03-c6c24b558288",
   "metadata": {},
   "source": [
    "##Geplante Datenquellen\n",
    "- Flatfiles\n",
    "- API von ESA\n",
    "- https://climate.esa.int/de/odp/#/dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f3459c-346a-4c3e-8311-c3de27c6625d",
   "metadata": {},
   "source": [
    "##Geplante Datensicherung\n",
    "- GIT Repository\n",
    "- Datenbanksicherung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0676c078-0af0-409f-a1dd-5259f0010faa",
   "metadata": {},
   "source": [
    "##Geplante Vorgehensweise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dcbb0a-e806-48ed-8807-6050642833d1",
   "metadata": {},
   "source": [
    "###Schritt 1: Datenfindung\n",
    "Recherche von ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68504ed3-aeca-4c5e-a3d4-6dab119cb20c",
   "metadata": {},
   "source": [
    "###Schritt 2: Datenbeschaffung\n",
    "Mindestens 2 verschiedene Datenquellen, die miteinander verbunden sind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4ca255-8b3a-45a0-96c7-fe548c7511eb",
   "metadata": {},
   "source": [
    "###Schritt 3: Datenanalyse\n",
    "Die Daten wollen wir in eine Datenbank spielen. Eventuell CouchDB. Die Daten werden wir mit Kafka bereitstellen und für die Analyse wird dann SparkSQL verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51bb136-e94b-4963-90eb-3602cef3992a",
   "metadata": {},
   "source": [
    "###Schritt 4: Ergebnisdarstellung\n",
    "Die erwarteten Ergebnisse werden mit Grafiken veranschaulicht. Diese werden wir mit Phyton erstellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e04a6-51ae-47d4-917f-0888fc8aa17d",
   "metadata": {},
   "source": [
    "##Weitere Eckpunkte\n",
    "- Speichern / Lesen / Verarbeiten der Daten mit einer Datenbank (relational oder NoSQL)\n",
    "- Kafka für die Bereitstellung der Daten verwenden (zumindest einen Teil, Kafka Producer, Kafka Consumer)\n",
    "- Spark für Datenhandling/analyse verwenden.\n",
    "- Abbildung von MapReduce mit Spark RDDs.\n",
    "- SparkSQL mindestens einmal verwenden.\n",
    "- Spark Dataframes verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652134a7-5b48-4ae4-85fe-126be0329225",
   "metadata": {},
   "source": [
    "##Meilensteine\n",
    "1. 14.12.2022: Präsentation des Themas und der Vorgehensweise.\n",
    "2. 21.12.2022: Zwischenabgabe des Projekts und Feedback-Einholung.\n",
    "3. 25.01.2023: Abgabe und Präsentation der Endergebnisse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a1da8-2a76-4564-be41-4a7d4eb1d532",
   "metadata": {},
   "source": [
    "##Erwarteter Output\n",
    "Grafiken..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fc544-0b3e-4872-8ab6-54cd50d60d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace2234b-3cf1-4afc-99f5-a140ece7e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install couchdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c31cde60-41cc-43f0-ade8-b7a290eacbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "from kafka import KafkaConsumer\n",
    "from pymongo import MongoClient\n",
    "from json import loads\n",
    "import couchdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf161fe-6739-4dd0-afaf-db65d866f426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"current_u\": {\n",
      "        \"units\": \"m/s\",\n",
      "        \"value\": -0.23236976563930511\n",
      "    },\n",
      "    \"current_v\": {\n",
      "        \"units\": \"m/s\",\n",
      "        \"value\": -0.3403078317642212\n",
      "    },\n",
      "    \"point\": {\n",
      "        \"depth\": 30.0,\n",
      "        \"latitude\": -34.04350280761719,\n",
      "        \"longitude\": 151.977294921875\n",
      "    },\n",
      "    \"salinity\": {\n",
      "        \"units\": \"g/kg\",\n",
      "        \"value\": 35.5641975402832\n",
      "    },\n",
      "    \"temperature\": {\n",
      "        \"units\": \"deg C\",\n",
      "        \"value\": 23.390777587890625\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "url = \"https://ocean.amentum.io/rtofs\"\n",
    "\n",
    "headers = {\"API-Key\": \"SSDqrYzBLYFDuO0iEnCKnYWdQLWNcGSO\"}\n",
    "\n",
    "params = {\n",
    "  \"latitude\": -34.0,\n",
    "  \"longitude\": 152.0,\n",
    "  \"depth\": 30\n",
    "}\n",
    "\n",
    "# handle exceptions\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "json_payload = response.json()\n",
    "\n",
    "print(json.dumps(json_payload, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70db20de-d98a-44f0-b78f-3b79d57168ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnrecognizedBrokerVersion",
     "evalue": "UnrecognizedBrokerVersion",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnrecognizedBrokerVersion\u001b[0m                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m topic_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m servers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost:29092\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# has to be adapted\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m producer \u001b[38;5;241m=\u001b[39m \u001b[43mKafkaProducer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbootstrap_servers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mvalue_serializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\kafka\\producer\\kafka.py:381\u001b[0m, in \u001b[0;36mKafkaProducer.__init__\u001b[1;34m(self, **configs)\u001b[0m\n\u001b[0;32m    378\u001b[0m reporters \u001b[38;5;241m=\u001b[39m [reporter() \u001b[38;5;28;01mfor\u001b[39;00m reporter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric_reporters\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics \u001b[38;5;241m=\u001b[39m Metrics(metric_config, reporters)\n\u001b[1;32m--> 381\u001b[0m client \u001b[38;5;241m=\u001b[39m KafkaClient(metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics, metric_group_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproducer\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    382\u001b[0m                      wakeup_timeout_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_block_ms\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    383\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[0;32m    385\u001b[0m \u001b[38;5;66;03m# Get auto-discovered version from client if necessary\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\kafka\\client_async.py:244\u001b[0m, in \u001b[0;36mKafkaClient.__init__\u001b[1;34m(self, **configs)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    243\u001b[0m     check_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version_auto_timeout_ms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m--> 244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_timeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\kafka\\client_async.py:909\u001b[0m, in \u001b[0;36mKafkaClient.check_version\u001b[1;34m(self, node_id, timeout, strict)\u001b[0m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    908\u001b[0m     remaining \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 909\u001b[0m     version \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbootstrap_topics_filter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# cache the api versions map if it's available (starting\u001b[39;00m\n\u001b[0;32m    912\u001b[0m         \u001b[38;5;66;03m# in 0.10 cluster version)\u001b[39;00m\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_versions \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mget_api_versions()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\kafka\\conn.py:1297\u001b[0m, in \u001b[0;36mBrokerConnection.check_version\u001b[1;34m(self, timeout, strict, topics)\u001b[0m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1296\u001b[0m     reset_override_configs()\n\u001b[1;32m-> 1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Errors\u001b[38;5;241m.\u001b[39mUnrecognizedBrokerVersion()\n\u001b[0;32m   1299\u001b[0m reset_override_configs()\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m version\n",
      "\u001b[1;31mUnrecognizedBrokerVersion\u001b[0m: UnrecognizedBrokerVersion"
     ]
    }
   ],
   "source": [
    "# connect to Kafka and open producer\n",
    "group_name = \"rtofs\"\n",
    "topic_name = \"pos1\"\n",
    "servers = ['localhost:29092']  # has to be adapted\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=servers, \n",
    "                         value_serializer=lambda x:dumps(x).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72ec914c-0b63-4918-809f-7cf0321c6f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data sent to topic pos1  blabla2\n"
     ]
    }
   ],
   "source": [
    "# write data into Kafka in a loop\n",
    "#for e in range(1000):\n",
    "#    data = {'number' : e}\n",
    "#    producer.send(topic_name, value=data)\n",
    "#    print (\"data sent to topic \"+topic_name+\" \",data)\n",
    "#    sleep(5)\n",
    "#data = json_payload\n",
    "data = \"blabla2\"\n",
    "producer.send(topic_name, value=data)\n",
    "print (\"data sent to topic \"+topic_name+\" \",data)\n",
    "sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d470c52-c673-48cf-9d63-19f40bd6a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close connection\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f32dd400-ca69-4ccb-bf82-5035d399645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "    'pos1',\n",
    "     bootstrap_servers=['localhost:29092'],\n",
    "     auto_offset_reset='earliest',\n",
    "     enable_auto_commit=True,\n",
    "     group_id='rtofs',\n",
    "     value_deserializer=lambda x: loads(x.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d13644b2-8757-4ebf-ab3c-883da3681109",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in consumer:\n",
    "    message = message.value\n",
    "    #collection.insert_one(message)\n",
    "    #print('{} added to {}'.format(message, collection))\n",
    "    print(message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140cfd2-f106-4e01-b204-d60200797f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a84139a3-ade1-49d5-95d3-d5aebc827bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "couch = couchdb.Server('http://admin:YOURPASSWORD@localhost:5984/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0366a91a-8914-4351-addf-3e60d82ae2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#db = couch.create('test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
